{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Working with Enriched Events data in [Google Colab](https://colab.research.google.com/) using Spark SQL\n",
        "\n",
        "In this Lab we'll learn how to work with [Enriched Event data](https://docs.developers.optimizely.com/web/docs/enriched-events-export) using [PySpark](http://spark.apache.org/docs/latest/api/python/index.html) and [Spark SQL](http://spark.apache.org/sql/).  \n",
        "\n",
        "This notebook is designed to be run in [Google Colab](https://colab.research.google.com/).  With that in mind, this notebook includes shell commands that install necessary prerequisites and download Optimizely enriched event data.  It is divided into the following sections:\n",
        "1. Install prerequisites\n",
        "2. Set global parameters\n",
        "3. Create a Spark session\n",
        "4. Load enriched event data\n",
        "5. Query enriched event data\n",
        "6. Some useful queries\n",
        "\n",
        "The last section contains a set of simple, useful queries for working with this dataset.  These queries can help you answer questions like\n",
        "- How many visitors were tested in my experiment?\n",
        "- How many \"unique conversions\" of an event were attributed to this variation?\n",
        "- What is the total revenue attributed to this variation?\n",
        "\n",
        "These queries are mirrored in the [Query Enriched Event Data with Spark](https://www.optimizely.com/labs/query-enriched-event-data-with-spark/) lab."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Install prerequisites\n",
        "\n",
        "This notebook relies on several external libraries and tools.  We'll start by installing them in our environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install pyspark\n",
        "! pip install pyspark\n",
        "\n",
        "# Install the AWS CLI\n",
        "! pip install awscli\n",
        "\n",
        "# Install jq (https://stedolan.github.io/jq/)\n",
        "# Note: this requires write permissions to /usr/local/bin\n",
        "! curl -L https://github.com/stedolan/jq/releases/download/jq-1.6/jq-linux64 -o /usr/local/bin/jq\n",
        "! chmod +x /usr/local/bin/jq\n",
        "\n",
        "# Install oevents (https://github.com/optimizely/oevents)\n",
        "# Note: this requires write permissions to /usr/local/bin\n",
        "! curl https://library.optimizely.com/data/oevents/latest/oevents -o /usr/local/bin/oevents\n",
        "! chmod +x /usr/local/bin/oevents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Set Global Parameters\n",
        "\n",
        "In this section we'll define a set of global parameters for our analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from getpass import getpass\n",
        "\n",
        "# These will determine the date range (inclusive) for which Optimizely Enriched Event data is downloaded\n",
        "ENRICHED_EVENTS_LOAD_START_DATE = \"2020-09-14\"\n",
        "ENRICHED_EVENTS_LOAD_END_DATE = \"2020-09-14\"\n",
        "\n",
        "# Determines the path into which the oevents CLI tool will download Optimizely Enriched Event data\n",
        "OPTIMIZELY_DATA_DIR = \"./optimizely_data\"\n",
        "\n",
        "# Enables oevents to authenticate to Optimizely\n",
        "# When you run this cell, you'll be asked to input a masked value\n",
        "# If this value is left blank, a read-only token for an Optimizely demo account will be used\n",
        "OPTIMIZELY_API_TOKEN = getpass(\"Enter your Optimizely API token\")\n",
        "\n",
        "# The default token provided here is a read-only token associated with a demo Optimizely account\n",
        "if OPTIMIZELY_API_TOKEN == \"\":\n",
        "    OPTIMIZELY_API_TOKEN = \"2:d6K8bPrDoTr_x4hiFCNVidcZk0YEPwcIHZk-IZb5sM3Q7RxRDafI\"\n",
        "\n",
        "print(f\"OPTIMIZELY_API_TOKEN={OPTIMIZELY_API_TOKEN[:10]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Some of the queries below make use of an _analysis window_.  That is, they include only events whose `timestamp` value falls between `ANALYSIS_START` and `ANALYSIS_END`.\n",
        "\n",
        "You may enter these values as datetime strings (UTC), or you can use the `optimizely_timestamp_to_datetime_str` helper function to convert the `beginDate` and `endDate` query parameter values from an Optimizely results page* into datetime strings.  The default values are taken from the [results page](https://app.optimizely.com/v2/projects/15783810061/results/18811053836/experiments/18786493712?share_token=e3c5fbc2e876a916f507671a125a3294faa65d4cc07d782854213e912731f730&previousView=VARIATIONS&variation=email_button&utm_campaign=copy&beginDate=1600107567520&endDate=1600115740325) for a demo Optimizely experiment.\n",
        "\n",
        "*`beginDate` and `endDate` are included in the URL of an Optimizely experiment results page when a \"Date Range\" is specified on the page."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "def optimizely_timestamp_to_datetime_str(timestamp_ms, round_down=False):\n",
        "  \"\"\"Conver the startDate and endDate values in an Optimizely results page URL to datetime strings\n",
        "  \n",
        "  Paramaters\n",
        "  ----------\n",
        "    timestamp_ms - a unix timestamp (ms) value\n",
        "    round_down - if true the result will be rounded down to the nearest 5 minute mark\n",
        "                 this is useful because the endDate value in the Optimizely results page\n",
        "                 is rounded down to the nearest 5-minute mark\n",
        "  \"\"\"\n",
        "  timestamp_s = timestamp_ms / 1000\n",
        "  if round_down:\n",
        "    timestamp_s = timestamp_s - (timestamp_s % (5*60))\n",
        "  dt = datetime.fromtimestamp(timestamp_s)\n",
        "  return dt.strftime('%Y-%m-%d %H:%M:%S')\n",
        "\n",
        "# The analysis window for your queries.  Change these values if you wish to restrict the event data included in your\n",
        "# queries\n",
        "ANALYSIS_START =  optimizely_timestamp_to_datetime_str(1600107567520)\n",
        "ANALYSIS_END = optimizely_timestamp_to_datetime_str(1600115740325, True)\n",
        "\n",
        "# Uncomment the following if you prefer to specify start and end times as strings\n",
        "# ANALYSIS_START = \"2020-09-14 11:19:27\"\n",
        "# ANALYSIS_END = \"2020-09-14 13:35:00\"\n",
        "\n",
        "print(f\"Analysis window: {ANALYSIS_START} to {ANALYSIS_END}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Below we'll use the [oevents](https://github.com/optimizely/oevents) CLI to download Optimizely Enriched Event data.  This tool relies on the following environment variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "\n",
        "# Set envars for oevents\n",
        "%env OPTIMIZELY_API_TOKEN={OPTIMIZELY_API_TOKEN}\n",
        "%env OPTIMIZELY_DATA_DIR={OPTIMIZELY_DATA_DIR}\n",
        "\n",
        "# Clear output so the API token is not stored in plaintext\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Creating a Spark Session\n",
        "\n",
        "Borrowed in part from the [Spark SQL getting started guide](https://spark.apache.org/docs/latest/sql-getting-started.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName(\"Python Spark SQL\") \\\n",
        "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \\\n",
        "    .config(\"spark.sql.repl.eagerEval.truncate\", 100) \\\n",
        "    .getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Loading Enriched Event data\n",
        "\n",
        "In this section we'll use the [oevents](https://github.com/optimizely/oevents) CLI to download Optimizely [Enriched Event data](https://docs.developers.optimizely.com/web/docs/enriched-events-export) from [Amazon S3](https://aws.amazon.com/s3/). Then we'll load this data into Spark Dataframes so that it can be queried and transformed.\n",
        "\n",
        "Enriched Event data is partitioned into two distinct datasets: [decisions](https://docs.developers.optimizely.com/optimizely-data/docs/enriched-events-data-specification#decisions-2) and [conversions](https://docs.developers.optimizely.com/optimizely-data/docs/enriched-events-data-specification#conversions-2).\n",
        "\n",
        "We'll start with [decision](https://docs.developers.optimizely.com/optimizely-data/docs/enriched-events-data-specification#decisions-2) data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "! oevents load --type decisions --start {ENRICHED_EVENTS_LOAD_START_DATE} --end {ENRICHED_EVENTS_LOAD_END_DATE}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next we'll load [conversion](https://docs.developers.optimizely.com/optimizely-data/docs/enriched-events-data-specification#conversions-2) data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "! oevents load --type events --start {ENRICHED_EVENTS_LOAD_START_DATE} --end {ENRICHED_EVENTS_LOAD_END_DATE}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that we've downloaded our data to local disk, we're going to load it into [Spark Dataframes](https://spark.apache.org/docs/latest/sql-programming-guide.html)\n",
        "\n",
        "Let's define a helper function for reading enriched event data into Spark Dataframes.  This function \n",
        "  1. Loads all parquet data in the specified directory into a Spark Dataframe\n",
        "  2. Uses the `uuid` field to remove any duplicate events from the loaded data\n",
        "  3. Filters all events with `timestamp` outside of our analysis window\n",
        "  4. Creates a new temporary view and returns a reference to the Dataframe "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "def view_exists(spark_session, view_name):\n",
        "    \"\"\"Return True if the specified view exists\"\"\"\n",
        "    try:\n",
        "        _ = spark_session.read.table(view_name)\n",
        "        return True\n",
        "    except:\n",
        "        return False\n",
        "\n",
        "def read_parquet_data_from_disk(\n",
        "    spark_session, \n",
        "    data_path, \n",
        "    view_name, \n",
        "    drop_uuid_duplicates=True,\n",
        "    replace_existing_view=True,\n",
        "    timestamp_window=None,\n",
        "):\n",
        "    \"\"\"Read parquet data from the specified directory into a temporary view\n",
        "    \n",
        "    Parameters:\n",
        "        spark_session -- SparkSession object\n",
        "        data_path -- string specifying the path on disk of the data to be read\n",
        "        view_name -- string name for the spark view to be created\n",
        "        replace_existing_view -- if True, a pre-existing view with the specified name will be\n",
        "                                 replaced by the newly read data\n",
        "        timestamp_window -- A 2-tuple of datetime strings indicating a time window. Records\n",
        "                            whose timestamp field falls outside of this window will be filtered\n",
        "        drop_duplicates -- if true, ensures that uuid values are unique in the resulting dataframe\n",
        "    \"\"\"\n",
        "    \n",
        "    if not replace_existing_view and view_exists(spark_session, view_name):\n",
        "        return\n",
        "    \n",
        "    df = spark_session.read.parquet(data_path)\n",
        "    \n",
        "    if drop_uuid_duplicates:\n",
        "        df = df.dropDuplicates(subset=[\"uuid\"])\n",
        "    \n",
        "    if timestamp_window is not None:\n",
        "        min_timestamp, max_timestamp = timestamp_window\n",
        "        df = df.filter(F.col(\"timestamp\").between(min_timestamp, max_timestamp))\n",
        "\n",
        "    df.createOrReplaceTempView(view_name)\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We'll load decision data from the `type=decisions` directory in our `OPTIMIZELY_DATA_DIR`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "decisions = read_parquet_data_from_disk(\n",
        "    spark_session=spark,\n",
        "    data_path=os.path.join(OPTIMIZELY_DATA_DIR, \"type=decisions\"),\n",
        "    view_name=\"decisions\",\n",
        "    timestamp_window=(ANALYSIS_START, ANALYSIS_END)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We'll load conversion data from the `type=events` directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "conversions = read_parquet_data_from_disk(\n",
        "    spark_session=spark,\n",
        "    data_path=os.path.join(OPTIMIZELY_DATA_DIR, \"type=events\"),\n",
        "    view_name=\"events\",\n",
        "    timestamp_window=(ANALYSIS_START, ANALYSIS_END)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Querying our data\n",
        "\n",
        "Now that we've loaded our data, we can query it using the `sql()` function.  Here's an example on our `decisions` view:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "spark.sql(\"\"\"\n",
        "    SELECT\n",
        "        *\n",
        "    FROM decisions\n",
        "    LIMIT 1\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here's an example on our `events` view:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "spark.sql(\"\"\"\n",
        "    SELECT\n",
        "        *\n",
        "    FROM events\n",
        "    LIMIT 1\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Useful queries\n",
        "\n",
        "Next we'll cover some simple, useful queries for working with Optimizely's Enriched Event data. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Counting the unique visitors in an Optimizely Web experiment \n",
        "\n",
        "[Optimizely Web]: https://www.optimizely.com/platform/experimentation/\n",
        "[Optimizely Full Stack]: https://docs.developers.optimizely.com/full-stack/docs\n",
        "\n",
        "[Optimizely Web] and [Optimizely Full Stack] experiment results pages count unique visitors in slightly different ways.  \n",
        "\n",
        "Given a particular analysis time window (between `start` and `end`) [Optimizely Web] attributes all visitors who were exposed to a variation at any time between when the experiment started and `end` and sent _any_ event (decision or conversion) to Optimizely between `start` and `end`.\n",
        "\n",
        "The following query captures that attribution logic:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Count the unique visitors from all events (Optimizely Web)\n",
        "\n",
        "spark.sql(f\"\"\"\n",
        "    SELECT \n",
        "        experiment_id,\n",
        "        variation_id,\n",
        "        COUNT (distinct visitor_id) as `Unique visitors (Optimizely Web)`\n",
        "    FROM (\n",
        "        SELECT\n",
        "            exp.experiment_id as experiment_id,\n",
        "            exp.variation_id as variation_id,\n",
        "            visitor_id\n",
        "        FROM events\n",
        "        LATERAL VIEW explode(experiments) t AS exp\n",
        "        UNION\n",
        "        SELECT\n",
        "            experiment_id,\n",
        "            variation_id,\n",
        "            visitor_id\n",
        "        FROM decisions\n",
        "        WHERE\n",
        "            is_holdback = false\n",
        "        )\n",
        "    GROUP BY\n",
        "        experiment_id,\n",
        "        variation_id\n",
        "    ORDER BY\n",
        "        experiment_id ASC,\n",
        "        variation_id ASC\n",
        "\"\"\").toPandas()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**A note on `timestamp` vs `process_timestamp`:** If you're working on re-computing the numbers you see on your [experiment results page](https://help.optimizely.com/Analyze_Results/The_Experiment_Results_page_for_Optimizely_X), it's important to understand the difference between the `timestamp` and `process_timestamp` fields in your Enriched Events data.\n",
        "\n",
        "- `timestamp` contains the time set by the _client_, e.g. the Optimizely Full Stack SDK\n",
        "- `process_timestamp` contains the approximate time that the event payload was received by Optimizely\n",
        "\n",
        "The difference is important because Enriched Event data is partitioned by `process_timestamp`, but Optimizely results are computed using `timestamp`.  This allows clients to send events retroactively, but also means that depending on your implementation you may need to load a wider range of data in order to ensure that you've captured all of the events with a `timestamp` in your desired analysis range."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Counting the unique visitors in an Optimizely Full Stack experiment \n",
        "\n",
        "[Optimizely Web]: https://www.optimizely.com/platform/experimentation/\n",
        "[Optimizely Full Stack]: https://docs.developers.optimizely.com/full-stack/docs\n",
        "\n",
        "The [Full Stack][Optimizely Full Stack] attribution model is a little simpler:\n",
        "\n",
        "Given a particular analysis time window (between `start` and `end`) [Full Stack][Optimizely Full Stack] attributes all visitors who were exposed to a variation at any time between `start` and `end`.  We measure this by counting the unique `visitor_id`s in the decisions dataset for that experiment:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Count the unique visitors from decisions (Optimizely Full Stack)\n",
        "\n",
        "spark.sql(f\"\"\"\n",
        "    SELECT\n",
        "        experiment_id,\n",
        "        variation_id,\n",
        "        COUNT(distinct visitor_id) as `Unique visitors (Full Stack)`\n",
        "    FROM decisions\n",
        "    WHERE\n",
        "        is_holdback = false\n",
        "    GROUP BY\n",
        "        experiment_id,\n",
        "        variation_id\n",
        "    ORDER BY\n",
        "        experiment_id ASC,\n",
        "        variation_id ASC\n",
        "\"\"\").toPandas()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Counting conversions in an Optimizely Web experiment\n",
        "\n",
        "[Optimizely Web]: https://www.optimizely.com/platform/experimentation/\n",
        "[Optimizely Full Stack]: https://docs.developers.optimizely.com/full-stack/docs\n",
        "\n",
        "When it comes to counting conversions, [Optimizely Full Stack] and [Optimizely Web] do things a little differently.\n",
        "\n",
        "Given a particular analysis time window (between `start` and `end`) [Optimizely Web] will attribute an event to a particular variation if the visitor who triggered that event was exposed to the variation at any time prior to that event, _even if it was before the beginning of the analysis time window._\n",
        "\n",
        "Optimizely event data is enriched with a an attribution column, `experiments`, that lists all of the experiments and variations to which an event has been attributed. Since Optimizely Web does not require that a corresponding decision take place during the analysis window, we can use a simple query to count the number of attributed conversions during our analysis window."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Count the unique conversions of a particular event attributed to an experiment\n",
        "\n",
        "spark.sql(f\"\"\"\n",
        "    SELECT \n",
        "        exp.experiment_id as experiment_id,\n",
        "        exp.variation_id as variation_id,\n",
        "        event_name,\n",
        "        COUNT(1) as `Conversion count (Optimizely Web)`\n",
        "    FROM events\n",
        "    LATERAL VIEW explode(experiments) t AS exp\n",
        "    GROUP BY\n",
        "        experiment_id, variation_id, event_name\n",
        "    ORDER BY\n",
        "        experiment_id ASC,\n",
        "        variation_id ASC,\n",
        "        event_name ASC\n",
        "\"\"\").toPandas()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Counting conversions in an Optimizely Full Stack experiment\n",
        "\n",
        "[Optimizely Web]: https://www.optimizely.com/platform/experimentation/\n",
        "[Optimizely Full Stack]: https://docs.developers.optimizely.com/full-stack/docs\n",
        "\n",
        "Given a particular analysis time window (between `start` and `end`) [Optimizely Full Stack] will attribute an event to a particular variation if the visitor who triggered that event was exposed to the variation prior to that event and _during the analysis window._\n",
        "\n",
        "Since Optimizely Full Stack requires that a corresponding decision take place during the analysis window, the query required to attribute events to experiments and variation is more complex."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "spark.sql(f\"\"\"\n",
        "    SELECT \n",
        "        experiment_id,\n",
        "        variation_id,\n",
        "        event_name,\n",
        "        COUNT (1) as `Conversion count (Optimizely Full Stack)`\n",
        "    FROM (\n",
        "         SELECT \n",
        "             d.experiment_id,\n",
        "             d.variation_id,\n",
        "             e.event_name,\n",
        "             e.visitor_id\n",
        "         FROM events e\n",
        "         INNER JOIN \n",
        "         (\n",
        "            SELECT \n",
        "                experiment_id,\n",
        "                variation_id,\n",
        "                visitor_id,\n",
        "                MIN(timestamp) as decision_timestamp\n",
        "            FROM decisions\n",
        "            WHERE \n",
        "                is_holdback = false\n",
        "            GROUP BY\n",
        "                experiment_id,\n",
        "                variation_id,\n",
        "                visitor_id\n",
        "         ) d \n",
        "         ON e.visitor_id = d.visitor_id\n",
        "         WHERE\n",
        "             e.timestamp >= d.decision_timestamp\n",
        "    )\n",
        "    GROUP BY\n",
        "         experiment_id,\n",
        "         variation_id,\n",
        "         event_name\n",
        "    ORDER BY\n",
        "        experiment_id ASC,\n",
        "        variation_id ASC\n",
        "\"\"\").toPandas()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## How to run this notebook\n",
        "\n",
        "This notebook lives in the [Optimizely Labs](http://github.com/optimizely/labs) repository.  You can download it and everything you need to run it by doing one of the following\n",
        "- Downloading a zipped copy of this Lab directory on the [Optimizely Labs page](https://www.optimizely.com/labs/computing-experiment-subjects/)\n",
        "- Downloading a [zipped copy of the Optimizely Labs repository](https://github.com/optimizely/labs/archive/master.zip) from Github\n",
        "- Cloning the [Github respository](http://github.com/optimizely/labs)\n",
        "\n",
        "Once you've downloaded this Lab directory (on its own, or as part of the [Optimizely Labs](http://github.com/optimizely/labs) repository, follow the instructions in the `README.md` file for this Lab."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "optimizelylabs",
      "language": "python",
      "display_name": "Python 3 (Optimizely Labs)"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
